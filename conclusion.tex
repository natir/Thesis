\documentclass[main.tex]{subfiles}

\begin{document}

\chapter{Conclusion}

In this work, we aimed to improve the long-read assembly pipeline without creating a new assembly tool. We have design tools that work before and after assembly. These tools can be easily integrated into a workflow, with besides the idea to improve assembly pipelines one tool at times.

Building pipelines with a collection of tools that make simple tasks helps to improve each specific task. It enhances the re-usability of each component, and the flexibility of the pipeline usage. Many assembly pipelines are a set of difficultly configurable black boxes, which does not help the user to adapt assembly tools to their own problem. Applying UNIX philosophy "Doing only one thing, and doing it well" on genome assembly could save the time of the community and improve results, as shown in the Hackseq 2018 Genome Assembler Components project\footnote{https://github.com/hackseq/modular-assembly-hs18}. Modular assembly should be the route to design versatile tools, able to be easily tuned to specific tasks, while understanding and keeping under control each step.

\fpa was created after a reflection on information generated by overlapping tools and its impact on disk space. Many overlaps are not useful for all analysis, for example \miniasm keeps only end-to-end overlaps, thus storing all overlaps found by \minimap on disk is a waste of disk space. Moreover, writing and reading these overlaps takes times. \fpa not only filters overlap, but also can rename reads in overlap (to reduce disk memory impact of overlaps), generates a \texttt{GFA1} overlap graph, -- or index the position of overlap in output file. This functionality was used by \consent \cite{CONSENT}. \fpa was used to avoid the necessity of write one's own filters, Erik Garrison uses it to simplify his work on \toolsname{seqwish}\footnote{https://github.com/ekg/seqwish}, a tools to create pangenome graph.

\yacrd uses coverage information as a proxy of reads region quality, it's a simple idea already present in correction tools. However, \yacrd extracts this functionally out of correction tools, increasing the modularity of pipelines. This increase helps to improve each step of the pipeline separately, to choose the relevant tools for specific data and analysis. A pre-publication version of \yacrd, with chimeric detection only, was used in a long read microbiota profiling pipeline to clean chimeric reads \cite{cite_yacrd} and to improve some \flye assemblies. 

Some improvements can be made on \yacrd pipeline. To detect bad quality regions, \yacrd uses \minimap with a specific parameter, to avoid the creation of a bridge between two good quality regions over the bad quality regions. A solution like \miniscrub was to use the seed position as a proxy of a quality region instead of the overlap, to directly avoid this trouble. Another solution was to replace minimizers by seed with error to find a similar region between reads over sequencing errors. Replacing \minimap by tools using seeds with error to estimate the coverage of reads regions, was probably improved by these tools. Some overlapping tools use this idea, like \toolsname{GroupK}\cite{GroupK}.

\yacrd having a very global point of view on the composition in base and the quality of the reads, avoid the problems of masking heterozygotie that can still be observed today in correctors. But the problem of the accentuation of coverage gaps by which we have been able to observe and solve with to \knot is potentially always present in \yacrd. Indeed, if we follow the recommended parameters and a region of the genome is sequenced at a depth of less than 3 \yacrd will create a coverage hole. If we want to avoid this problem we would need to have a broader analysis of the problem, not this focus on a single read at a time, potentially through the construction of local overlap graphs around the reads. This work can be apply to scrubbing and correction tools, but this change in perspective will probably take time and some many development to have equivalent performance of actual tools.

\knot is a tool to retrieve missing connections between contigs, because it's an important task \knot can do. \knot uses \minimap to find overlap between reads and between contigs, \yacrd to remove low quality reads from raw reads dataset and \fpa to filter overlaps and generate overlaps graph; before \knot script performs path search in this graph. The main idea behind \knot is that sometimes we have to consider all the data to solve the problem. At the moment, assembly pipelines try to keep only the minimum amount of information to solve the assembly problem (cf Chapter \ref{chapter:sota}) and this is a very good approach that allows to accelerate the assembly in a very important way. But sometimes, this reduction of information goes too far and important information is lost. \knot, by going back to the original information and focusing only on unresolved points, tries to correct these errors.

\bigskip

This idea to go back to the total information can be the main trouble of \knot.
The size of \knot \OLC was very important for example in Table \ref{conclusion:tab:AAG_building}. 
We can see two Nanopore datasets, one from \textit{E. coli} and one from \textit{D. melanogaster}. \textit{D. melanogaster} dataset is larger than \textit{E. coli} dataset, less than 10 times. But the computation time to build \knot \OLC from the overlaps found by minimap2 was increased by 30 times.

\begin{table}[]
    \centering
    \begin{tabular}{c|ll|ll|l}
        & &  & \multicolumn{2}{c|}{In \knot \OLC} & \knot \OLC \\
        & \# bases & \# reads & \# Nodes & \# Edges & construction time \\ \hline
        \textit{E. coli} & 1621000527 & 158590 & 24966 & 158590 & ~ 2 hours \\
        \textit{D. melanogaster} & 9064470438 & 1327569 & 234253 & 956929 & ~ 3 days \\ \hline 
        ratio & 5.59 & 8.37 & 9.38 & 6.03 & ~36 \\
    \end{tabular}
    \caption{A comparison of two Nanopore datasets. The ratio was computed by dividing \textit{D. melanogaster} value by \textit{E. coli} value. The size of data increases by less than an order of magnitude but the construction time increases more than 2 orders of magnitude.}
    \label{conclusion:tab:AAG_building}
\end{table}

To use \knot on a large dataset, we need to change how we use this graph.
Actually \knot loads all graphs in memory, however we don't need all this information to be permanently loaded into memory.
We could load only one part of the graph at a time. 
Another trouble with larger datasets concerns genome with more than one chromosomes, at this time we did not try to prevent the creation of false links between contigs for different chromosomes. To adapt \knot to larger genomes, we have to solve a technical problem on how to represent these very large graphs in memory.
We must also tackle a more theoretical problem of how to ensure that we do not create links between contigs of different chromosomes.

\knot needs to be updated to be run easily on a larger datasets, but \knot idea can also lead to more features. We use \knot \OLC to refund the lost link between contigs, we focus our analysis of graph on contigs extremities. But by performing some graph analysis along of the contigs, we can maybe detect misassemblies.
To do this, we can draw inspiration from \canu repetition detection module (see \ref{sota:fig:canu:remapping}) or something not to far to \toolsname{tigmint}\cite{jackman2018tigmint}.

The actual version of \knot has total confidence in the contigs, while the future evolution could maybe mark some spurious region or break contigs. Analysis of all reads information can lead to another extension. I think we can convert contigs and \knot \OLC information to a genome graph. A genome graph is a new type of genome representation, which replaces a linear representation of genome by a graph where each nucleotide is a node, and an edge is created if nodes follow in the genome. This type of structure could be useful to solve the limitations of reference genome approaches. 
For example \toolsname{WhatsHap} \cite{whatshap} needs to build an alternative local part of genome according to known variant to perform remapping of reading and confirm the read dataset contains effectively the reads dataset. A similar structure was used for genome comparison \toolsname{Cactus} \cite{cactus_graph}.

This type of structure seems promising for future bioinformatics analysis, variant detection and phasing, genome comparison,  genomics evolution, and variation analysis \cite{goodbye_ref_hello_graphs}. But some trouble still needs to be addressed: how to build this type of graph, and how to map reads against them to construct an efficient coordinate system.
Here are some blog post was you can read some blog post about part of this trouble \footnote{\url{http://ekg.github.io/2019/07/09/Untangling-graphical-pangenomics}} \footnote{\url{https://lh3.github.io/2019/07/08/on-a-reference-pan-genome-model}} \footnote{\url{https://lh3.github.io/2019/07/12/on-a-reference-pan-genome-model-part-ii}}. Another challenge that interests me a lot would be to be able to build a graph genome during assembly.

By using the contigs generated by assembly tools as scaffolds of a genome graph and \knot \OLC information, I think we can generate directly the genome graph from the reads. If reads come from a single homozygous individual, this genome graph does not contain any information, in theory. But for a heterozygous individual or a set of divergent cells like cancer cell or a bacterial population, this genome graph representation can help to have a better understanding of the sequenced genome.

\onlyinsubfile{
\bibliographystyle{plainnat}
\bibliography{main}
\addcontentsline{toc}{chapter}{Bibliography}
}

\end{document}
