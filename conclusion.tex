\documentclass[main.tex]{subfiles}

\begin{document}

\chapter{Conclusion}

In this work, we aimed to improve the long-read assembly pipeline without creating a new assembly tool. We have design tools that work before and after assembly. These tools can be easily integrated into a workflow, with besides the idea to improve assembly pipelines one tool at times.

Building pipelines with a collection of tools that make simple tasks helps to improve each specific task. It enhances the re-usability of each component, and the flexibility of the pipeline usage. Many assembly pipelines are a set of difficultly configurable black boxes, which does not help the user to adapt assembly tools to their own problem. Applying UNIX philosophy "Doing only one thing, and doing it well" on genome assembly could save the time of the community and improve results, as shown in the Hackseq 2018 Genome Assembler Components project\footnote{https://github.com/hackseq/modular-assembly-hs18}. Modular assembly should be the route to design versatile tools, able to be easily tuned to specific tasks, while understanding and keeping under control each step.

\fpa was created after a reflection on information generated by overlapping tools and its impact on disk space. Many overlaps are not useful for all analysis, for example \miniasm keeps only end-to-end overlaps, thus storing all overlaps found by \minimap on disk is a waste of disk space. Moreover, writing and reading these overlaps takes times. \fpa not only filters overlap, but also can rename reads in overlap (to reduce disk memory impact of overlaps), generates a \texttt{GFA1} overlap graph, -- or index the position of overlap in output file. This functionality was used by \consent \cite{CONSENT}. \fpa was used to avoid the necessity of write one's own filters, Erik Garrison uses it to simplify his work on \toolsname{seqwish}\footnote{https://github.com/ekg/seqwish}, a tools to create genome graph.

\yacrd uses coverage information as a proxy of reads region quality, it's a simple idea already present in correction tools. However, \yacrd extracts this functionally out of correction tools, increasing the modularity of pipelines. This increase helps to improve each step of the pipeline separately, to choose the relevant tools for specific data and analysis. A pre-publication version of \yacrd, with chimeric detection only, was used in a long read microbiota profiling pipeline to clean chimeric reads \cite{cite_yacrd} and to improve some \flye assemblies. 

Many improvements can be made on \yacrd pipeline. To detect bad quality regions, \yacrd uses \minimap with a specific parameter, to avoid the creation of a bridge between two good quality regions over the bad quality regions. A solution like \miniscrub was to use the seed position as a proxy of a quality region instead of the overlap, to directly avoid this trouble. Another solution was to replace minimizers by seed with error to find a similar region between reads over sequencing errors. Replacing \minimap by tools using seeds with error to estimate the coverage of reads regions, was probably improved by these tools. Some overlapping tools use this idea, like \toolsname{GroupK}\cite{GroupK}. 

\knot was a tool to retrieve missing connections between contigs, because it's an important task \knot can do. \knot uses \minimap to find overlap between reads and between contigs, \yacrd to remove low quality reads from raw reads dataset and \fpa to filter overlaps and generate overlaps graph; before \knot script performs path search in this graph. The main idea behind \knot is that sometimes we have to consider all the data to solve the problem. At the moment, assembly pipelines try to keep only the minimum amount of information to solve the assembly problem (cf Chapter \ref{chapter:sota}) and this is a very good approach that allows to accelerate the assembly in a very important way. But sometimes, this reduction of information goes too far and important information is lost. \knot, by going back to the original information and focusing only on unresolved points, tries to correct these errors.

\bigskip

This idea to go back to all information can be the main trouble of \knot, the size of \knot \OLC was very important for example in Table \ref{conclusion:tab:AAG_building} we can see two Nanopore dataset one from \textit{E. coli} one from \textit{D. melanogaster} we can see \textit{D. melanogaster} dataset is larger than \textit{E. coli} dataset, less than 10 times. But the computation time to build \knot \OLC from overlap found by minimap2 was increase by 30 times.

\begin{table}[]
    \centering
    \begin{tabular}{c|ll|ll|l}
        & &  & \multicolumn{2}{c|}{In \knot \OLC} & \knot \OLC \\
        & \# bases & \# reads & \# Nodes & \# Edges & construction time \\ \hline
        \textit{E. coli} & 1621000527 & 158590 & 24966 & 158590 & ~ 2 hours \\
        \textit{D. melanogaster} & 9064470438 & 1327569 & 234253 & 956929 & ~ 3 days \\ \hline 
        ratio & 5.59 & 8.37 & 9.38 & 6.03 & ~36 \\
    \end{tabular}
    \caption{A comparaison of two Nanopore dataset. Ratio was compute by divide \textit{D. melanogaster} value by \textit{E. coli} value. The size of data increase by less than an order of magnitude but the construction time increase more than 2 order of magnitude}
    \label{conclusion:tab:AAG_building}
\end{table}

To use \knot on a large dataset, we need to change how we use this graph, actually \knot load all graph in memory or we don't need all this information to be permanently loaded into memory we could load only part of the graph at a time. Another trouble with larger dataset concern genome with more than one chromosome, at this time we didn't try to prevent the creation of false links between contigs for different chromosomes. To adapt \knot to larger genomes we have to solve a technical problem on how to represent these very large graphs in memory, and this more theoretical problem of how to ensure that we do not create links between contigs of different chromosomes.

\knot need to be updated to be run easily on a larger dataset, but \knot idea can also lead to more features. We use \knot \OLC to refund the lost link between contigs we focus our analysis of graph on contigs extremity. But by performing some graph analysis along of the contigs we can maybe detect misassemblies, for this we can be inspired by \canu repetition detection module (see \ref{sota:fig:canu:remapping}) or something not to fare to \toolsname{tigmint}\cite{jackman2018tigmint}.

The actual version of \knot has total confidence in the contigs, in the future evolution can maybe mark some spurious region or break contigs. Analysis of all reads information can lead to another extension. I think we can convert contigs and \knot \OLC information to a genome graph. A genome graph was a new type of genome representation, they replace a linear representation of genome by a graph where each nucleotide was a node, and an edge created if they follow in the genome. This type of structure could be useful to solve the limitation of reference genome approach, for example \toolsname{WhatsHap} \cite{whatshap} needs to build an alternative local part of genome according to know variant to perform remapping of reading and confirmed the read dataset contains effectively the reads dataset. Similar structure was used for genome comparison \toolsname{Cactus} \cite{cactus_graph}.

This type of structure seems promising for future bioinformatics analysis, variant detection and phasing, genome comparison,  genomics evolution, and variation analysis \cite{goodbye_ref_hello_graphs}. But some trouble style need to be addressed, how to build this type of graph, map reads against them construct an efficient coordinate system, some blog post was you can read some blog post about part of this trouble \footnote{\url{http://ekg.github.io/2019/07/09/Untangling-graphical-pangenomics}} \footnote{\url{https://lh3.github.io/2019/07/08/on-a-reference-pan-genome-model}} \footnote{\url{https://lh3.github.io/2019/07/12/on-a-reference-pan-genome-model-part-ii}}. Another challenge that interests me a lot would be to be able to build a graph genome during assembly.

By using the contigs generate by assembly tools has scaffold of genome graph and \knot \OLC information I think we can generate directly the genome graph from the reads. If reads come from a single homozygote individual, this genome graph didn't contain any information, in theory, but for a heterozygote individual or a set of divergent cells like cancer or a bacterial population, this genome graph representation can help to have a better understanding of the sequenced genome.

\onlyinsubfile{
\bibliographystyle{plainnat}
\bibliography{main}
\addcontentsline{toc}{chapter}{Bibliography}
}

\end{document}
