\documentclass[main.tex]{subfiles}

\begin{document}

\chapter{Conclusion}

\yacrd use coverage information as a proxy of reads region quality, it's a simple idea already present in correction tools, but extract this functionally out of correction tools, increase modularity of pipeline. Increase the modularity of pipeline help to improve each step of pipeline separately to choose the good tools for specific data and analysis. A pre-publication version of \yacrd, with only chimeric detection, in a long read microbiota profiling pipeline to clean chimeric reads \cite{cite_yacrd}.

\fpa was created after a reflexion on information generated by overlapping tools and her impact on disk space, many of overlap aren't useful for all analysis per example \miniasm keep only end-to-end overlap, store all overlap found by \minimap on disk was a wast of disk space moreover writing and reading of this overlap take times. \fpa not only filter overlap they can rename reads in overlap (to reduce disk memory impact of overlap), generate a \texttt{GFA1} overlap graph, or index the position of overlap in output file this functionality was used by \consent \cite{CONSENT}. \fpa was used to avoid the necessity of write there owns filters, Erik Garrison uses it to simplify her work on \toolsname{seqwish}\footnote{https://github.com/ekg/seqwish} a tools to create genome graph.

These two tools aren't based on complex new algorithm, but they are easy to use and have a direct impact on result. Build pipeline with a collection of tools they make simple task help to improve each specific task and improve reusability of each component, and flexibility of pipeline usage. Many of assembly pipeline are actually a set of hard configurable black box, this doesn't help user to adapt assembly tools to there own problem. Apply UNIX philosophy "Doing only one thing, and doing it well" on genome assembly could save time of the community and improve results, as shown in the hackseq 2018 Genome Assembler Components project\footnote{https://github.com/hackseq/modular-assembly-hs18}.

\knot was not to fare to this idea, because it's a tool to perform one task, refound missing connection between contigs, but it uses many existing tools to perform is purpose. \knot use \minimap to found overlap between reads and between contigs, \yacrd to remove low quality reads from raw reads dataset and \fpa filter overlap and generate overlaps graph before \knot script performs path search in this graph. The main idea of \knot is sometimes we have to consider all the data to solve the problem. At present, assembly pipelines try to keep only the minimum amount of information to solve the assembly problem (cf Chapter \ref{chapter:sota}) and this is a very good approach that allows accelerating the assembly in a very important way. But sometimes this reduction of information goes too far and important information is lost. \knot by going back to the original information and focus only on not resolve point try to correct these errors.

To detect bad quality region \yacrd use \minimap with a specific parameter, to avoid the creation of a bridge between two good quality region over the bad quality region. A solution like \miniscrub was to use the seed position as a proxy of quality region in place of overlap directly avoid this trouble. Another solution was to replace minimizer by seed with error to found a similar region between reads over sequencing error. Replace \minimap by tools using seed with error to estimate the coverage of reads regions, was probably improve by these tools. Some overlapping tools use this idea, like \toolsname{GroupK}\cite{GroupK}.

Some improvement can be performed in \knot, by example, the search of a path between contigs extremity takes many times and can by easily improve, by replacing this part of the pipeline by using a compiled and paralyzed implementation of the path search algorithm. \knot use \minimap to detect containment contigs to didn't search path at her extremity, add this information in \knot report can useful for user. Moreover, add coverage information un AAG can help to make a decision in case of unsolved repetition like \flye.

\fpa was useful and is used in many ways including reducing information before a genome graph construction. A genome graph was a representation of genome they try to take into account all variants. We can roughly define a genome graph like a directed graph where each nucleotide was a node like by an edge if they exist in the genome, between each individual many variations exist with a genome graph we can represent these variations by adding node and edge in parallel of the other genome node.

This type of structure could be useful to solve the limitation of reference genome approach, for example \toolsname{WhatsHap} \cite{whatshap} needs to build an alternative local part of genome according to know variant to perform remapping of read and confirmed the read dataset contains effectively the reads dataset. Similar structure was used for genome comparison \toolsname{Cactus} \cite{cactus_graph}.

This type of structure seems promising for future bioinformatics analysis, variant detection and phasing, genome comparison,  genomics evolution, and variation analysis. But some trouble style need to be addressed, how to build this type of graph, map reads against them construct an efficient coordinate system, some blog post was you can read some blog post about part of this trouble \footnote{\url{http://ekg.github.io/2019/07/09/Untangling-graphical-pangenomics}} \footnote{\url{https://lh3.github.io/2019/07/08/on-a-reference-pan-genome-model}} \footnote{\url{https://lh3.github.io/2019/07/12/on-a-reference-pan-genome-model-part-ii}}. Another challenge that interests me a lot would be to be able to build a graph genome during assembly.


% Ouverture
%% Usage of small seed clustering with long read (scrubbing overlapping)
%% speed of knot scall up
%% repetition detection



\onlyinsubfile{
\bibliographystyle{plainnat}
\bibliography{main}
\addcontentsline{toc}{chapter}{Bibliography}
}

\end{document}