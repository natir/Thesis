\documentclass[main.tex]{subfiles}

\begin{document}

\chapter{Conclusion}

In this work, we aimed to improve the process of long-read genome assembly, without creating a new assembly tool. We have designed tools that work before and after assembly. These tools can be easily integrated into a workflow. The underlying idea is to improve assembly pipelines one tool at a time.

Building pipelines with a collection of tools that perform simple tasks, makes it easier to provide independent improvements to each task separately. It enhances the re-usability of each component, and the flexibility of the pipeline usage. Many assembly pipelines are a set of difficultly configurable black boxes, which does not help the user to adapt assembly tools to their own problem. Applying UNIX philosophy "Doing only one thing, and doing it well" on genome assembly could save the time of the community and improve results, as shown in the Hackseq 2018 Genome Assembler Components project\footnote{https://github.com/hackseq/modular-assembly-hs18}. Modular assembly should be the route to design versatile tools, able to be easily tuned to specific tasks, while understanding and keeping under control each step.

\fpa was created after a reflection on information generated by overlapping tools and its impact on disk space. Many overlaps are not useful for all analysis, for example \miniasm keeps only end-to-end overlaps, thus storing all overlaps found by \minimap on disk is a waste of disk space. Moreover, writing and reading these overlaps takes times. \fpa not only filters overlaps, but also can rename reads in overlap (to reduce disk memory impact of overlaps), generates a \texttt{GFA1} overlap graph, -- or index the position of overlap in output file. This functionality was used by \consent \cite{CONSENT}. \fpa was used to avoid the necessity of writing one's own filters, Erik Garrison uses it to simplify his work on \toolsname{seqwish}\footnote{https://github.com/ekg/seqwish}, a tools to create pangenome graph.

\yacrd uses coverage information as a proxy of reads region quality, it's a simple idea already present in correction tools. However, \yacrd extracts this functionally out of correction tools, increasing the modularity of pipelines. This helps to improve each step of the pipeline separately, to choose the relevant tools for specific data and analysis. A pre-publication version of \yacrd, with chimeric detection only, was used in a long read microbiota profiling pipeline to clean chimeric reads \cite{cite_yacrd} and to improve some \flye assemblies. 

Some improvements can be made on \yacrd pipeline. To detect bad quality regions, \yacrd uses \minimap with a specific parameter, to avoid the creation of a bridge between two good quality regions over the bad quality regions. A solution like \miniscrub was to use the seed position as a proxy of a quality region instead of the overlap, to directly avoid this trouble. Another solution was to replace minimizers by seed with error to find a similar region between reads over sequencing errors. Replacing \minimap by tools using seeds with error to estimate the coverage of reads regions, was probably improved by these tools. Some overlapping tools use this idea, like \toolsname{GroupK}\cite{GroupK}.

\yacrd takes a very global point of view on the composition in bases and the quality of the reads, avoiding the problems of masking heterozygosity that can still be observed today in correctors. But the problem of the accentuation of coverage gaps by which we have been able to observe and solve with to \knot is potentially always present in \yacrd. Indeed, if we follow the recommended parameters and a region of the genome is sequenced at a depth of less than 3 \yacrd will create a coverage hole. If we want to avoid this problem we would need to have a broader analysis of the problem, not this focus on a single read at a time, potentially through the construction of local overlap graphs around the reads. This work can be apply to scrubbing and correction tools, but this change in perspective will probably take time and some many development to have equivalent performance of actual tools.

\knot is a tool to retrieve missing connections between contigs. \knot uses \minimap to find overlap between reads and between contigs, \yacrd to remove low quality reads from raw reads dataset and \fpa to filter overlaps and generate overlaps graph; then a script in \knot performs path search within this graph. The main idea behind \knot is that sometimes we have to consider all the available data to solve a problem. At the moment, assembly pipelines try to keep only the minimum amount of information to solve the assembly problem (cf Chapter \ref{chapter:sota}) and this is a very good approach that allows to accelerate the assembly in a very important way. But sometimes, this reduction of information goes too far and important information is lost. \knot, by going back to the original information and focusing only on unresolved points, tries to correct these errors.

\bigskip

This idea to go back to the total information can however become a trouble for \knot.
The size of \knot overlap graph is very important for example in Table \ref{conclusion:tab:AAG_building}. 
We can see two Nanopore datasets, one from \textit{E. coli} and one from \textit{D. melanogaster}. \textit{D. melanogaster} dataset is larger than \textit{E. coli} dataset, less than 10 times. But the computation time to build \knot overlap graph from the overlaps found by \minimap was increased by 30 times.

\begin{table}[]
    \centering
    \begin{tabular}{c|ll|ll|l}
        & &  & \multicolumn{2}{c|}{In \knot graph} & \knot graph \\
        & \# bases & \# reads & \# Nodes & \# Edges & construction time \\ \hline
        \textit{E. coli} & 1621000527 & 158590 & 24966 & 158590 & ~ 2 hours \\
        \textit{D. melanogaster} & 9064470438 & 1327569 & 234253 & 956929 & ~ 3 days \\ \hline 
        ratio & 5.59 & 8.37 & 9.38 & 6.03 & ~36 \\
    \end{tabular}
    \caption{A comparison of two Nanopore datasets. The ratio was computed by dividing \textit{D. melanogaster} value by \textit{E. coli} value. The size of data increases by less than an order of magnitude but the construction time increases more than 2 orders of magnitude.}
    \label{conclusion:tab:AAG_building}
\end{table}

To use \knot on a large datasets, we need to change how we use this graph.
Currently \knot loads all graphs in memory, however we don't need all this information to be permanently loaded into memory.
We could load only one part of the graph at a time. 
Another trouble with larger datasets concerns genome with more than one chromosomes. At this time we did not try to prevent the creation of false links between contigs for different chromosomes. To adapt \knot to larger genomes, we have to solve a technical problem on how to represent these very large graphs in memory.
We must also tackle a more theoretical problem of how to ensure that we do not create links between contigs of different chromosomes.

If \knot needs to be updated to be run easily on larger datasets, ideas behind\knot can also lead to more features. We use \knot overlap graph to refund the lost link between contigs, we focus our analysis of graph on contigs extremities. But by performing some graph analysis along of the contigs, we can maybe detect misassemblies.
To do this, we can draw inspiration from \canu repetition detection module (see \ref{sota:fig:canu:remapping}) or something not to far to \toolsname{tigmint}\cite{jackman2018tigmint}.

The current version of \knot has total confidence in the contigs given as input, while the future evolution could may be mark some spurious region or break contigs. Analysis of all reads information can lead to another extension. I think we can convert contigs and \knot overlap graph information to a genome graph. A genome graph is a new type of genome representation, which replaces a linear representation of genome by a graph where each nucleotide is a node, and an edge is created if nodes follow in the genome. This type of structure could be useful to solve the limitations of reference genome approaches. 
For example \toolsname{WhatsHap} \cite{whatshap}, a tool to phase variant, perform a mapping of read against the reference. When \toolsname{WhatsHap} found a mismatch in mapping, he need to build a small new version of genome according variant database. \toolsname{WhatsHap} perform remapping of read against this new reference to confirm the read dataset contains effectively a known variation of this genome. A similar structure was used to perform genome comparison \toolsname{Cactus} \cite{cactus_graph}.

This type of structure seems promising for future bioinformatics analysis, variant detection and phasing, genome comparison,  genomics evolution, and variation analysis \cite{goodbye_ref_hello_graphs}. But some trouble still needs to be addressed: how to build this type of graph, and how to map reads against them to construct an efficient coordinate system.
Here are some blog post was you can read some blog post about part of this trouble \footnote{\url{http://ekg.github.io/2019/07/09/Untangling-graphical-pangenomics}} \footnote{\url{https://lh3.github.io/2019/07/08/on-a-reference-pan-genome-model}} \footnote{\url{https://lh3.github.io/2019/07/12/on-a-reference-pan-genome-model-part-ii}}. Another challenge that interests me a lot would be to be able to build a graph genome during assembly.

By using the contigs generated by assembly tools as scaffolds of a genome graph and \knot overlap graph  information, I think we can generate directly the genome graph from the reads. If reads come from a single homozygous individual, this genome graph does not contain variant information, in theory. But for a heterozygous individual or a set of divergent cells like cancer cell or a bacterial population, this genome graph representation can help to have a better understanding of the sequenced genome.

\section*{Summary of perspectives}

In the previous section we have summarized a number of elements of the thesis and detailed several improvements of our work. Here we would like to provide a more synthetic summary of the research perspectives opened by this work.

\paragraph{Overlaping consensus:} Overlapping search was a hard task, and perform it in reasonable time and memory usage was harder while many overlaps were missed. Combining information of different overlapping tools could be use full to improve downstream analysis.

To create this overlapping consensus tool we need to solve a technical problem: what is the best method to store and request this information. And even more theoretically, first, how to determine if we can merge these overlaps, and second how to assess the confidence we can have in resulting overlaps.

\paragraph{Scrub and correct reads without creating coverage gap:} Our work on \knot shows that sometimes the cleaning of reads create coverage gaps in reads. These gaps reduce the contiguity of assembly and reduce our confidence in contigs generated by assembly. At this moment, all trimming, scrubbing and correction tools work like a greedy algorithm, they focus on one read at time.

A read with high error rate and without support from other reads is probably not useful, but sometimes it can solve an assembly trouble. Spending time to find how to change the paradigm of these reads cleaning tools seems useful to me to maximize the usage of the data provided by the sequencing technology.

\paragraph{Find variant at assembly time:} We have indicated that the only long read corrector that tried to keep the heterozygosity of the reads during correction was \toolsname{falcon\_sense}. For a de novo assembly, we generally sequence individuals with as smallest heterozygosity as possible or a colony of the same cell, to facilitate our work during assembly.

Consequently, we build assembly tools that do not manage high heterozygosity or sets of cells with variants, like cancer cells and metagenomics datasets. Rewriting a complete assembly tools to manage data with variants seems very hard. The \knot strategy uses classic assembly tools to assemble simple parts of the genome, but going back to original information to find variants and heterozygosity seems a good way to find variant at assembly time.

\end{document}
