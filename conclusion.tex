\documentclass[main.tex]{subfiles}

\begin{document}

\chapter{Conclusion}

In this work, we aimed to improve long-read assembly pipeline without create a new assembly tools, we have design tools that work before, after assembly and this tools can be easly integrate in workflow. With idea to improve assembly pipeline one tools at times.

Build pipeline with a collection of tools they make simple task help to improve each specific task and improve reusability of each component, and flexibility of pipeline usage. Many of assembly pipeline are actually a set of hard configurable black box, this doesn't help user to adapt assembly tools to there own problem. Apply UNIX philosophy "Doing only one thing, and doing it well" on genome assembly could save time of the community and improve results, as shown in the hackseq 2018 Genome Assembler Components project\footnote{https://github.com/hackseq/modular-assembly-hs18}. Modular assembly should be the route to design versatile tools able to be easily tuned to specific tasks while understanting and keeping under control each step.

\fpa was created after a reflexion on information generated by overlapping tools and her impact on disk space, many of overlap aren't useful for all analysis per example \miniasm keep only end-to-end overlap, store all overlap found by \minimap on disk was a wast of disk space moreover writing and reading of this overlap take times. \fpa not only filter overlap they can rename reads in overlap (to reduce disk memory impact of overlap), generate a \texttt{GFA1} overlap graph, or index the position of overlap in output file this functionality was used by \consent \cite{CONSENT}. \fpa was used to avoid the necessity of write there owns filters, Erik Garrison uses it to simplify her work on \toolsname{seqwish}\footnote{https://github.com/ekg/seqwish} a tools to create genome graph.

\yacrd use coverage information as a proxy of reads region quality, it's a simple idea already present in correction tools, but extract this functionally out of correction tools, increase modularity of pipeline. Increase the modularity of pipeline help to improve each step of pipeline separately to choose the good tools for specific data and analysis. A pre-publication version of \yacrd, with only chimeric detection, was use in a long read microbiota profiling pipeline to clean chimeric reads \cite{cite_yacrd} and use to improve some \flye assemblies. 

Many improvement can be made on \yacrd pipeline, to detect bad quality region \yacrd use \minimap with a specific parameter, to avoid the creation of a bridge between two good quality region over the bad quality region. A solution like \miniscrub was to use the seed position as a proxy of quality region in place of overlap directly avoid this trouble. Another solution was to replace minimizer by seed with error to found a similar region between reads over sequencing error. Replace \minimap by tools using seed with error to estimate the coverage of reads regions, was probably improve by these tools. Some overlapping tools use this idea, like \toolsname{GroupK}\cite{GroupK}. 

\knot was a tools to refound missing connection between contigs because it's an important task \knot can do. \knot use \minimap to found overlap between reads and between contigs, \yacrd to remove low quality reads from raw reads dataset and \fpa filter overlap and generate overlaps graph before \knot script performs path search in this graph. The main idea of \knot is sometimes we have to consider all the data to solve the problem. At present, assembly pipelines try to keep only the minimum amount of information to solve the assembly problem (cf Chapter \ref{chapter:sota}) and this is a very good approach that allows accelerating the assembly in a very important way. But sometimes this reduction of information goes too far and important information is lost. \knot by going back to the original information and focus only on not resolve point try to correct these errors.

\bigskip

This idea to go back to all information can be the main trouble of \knot, the size of \knot \OLC was very important for example for a dataset Nanopore \textit{E. coli} coverage 
To use \knot on large dataset, we need to change how we use this graph, actually \knot load all graph in memory or we don't need all this information to be permanently loaded into memory we could load only part of the graph at a time. Another trouble with larger dataset concern genome with more than one chromosome, at this time we didn't try to prevent the creation of false link between contigs for different chromosome. To adapt \knot to larger genomes we have to solve a technical problem how to represent these very large graphs in memory, and this more theoretical problem of how to ensure that we do not create links between contigs of different chromosomes.

\knot need to be update to be run easly on larger dataset, but \knot idea can also lead to more feature. We use \knot \OLC to refund lost link between contigs we focus our analysis of graph on contigs extremity. But by perform some graph analysis along of the contigs we can maybe detect misassemblies, for this we can be inspired by \canu repetition detection module (see \ref{sota:fig:canu:remapping}) or something not to fare to \toolsname{tigmint}\cite{jackman2018tigmint}.

Actual version of \knot have a total confidence in the contigs, in the future evolution can maybe mark some spurious region or break contigs. Analysis of all reads information can lead to another extension. I think we can convert contigs and \knot \OLC information to a genome graph. A genome graph was a new type of genome representation, they replace a linear representation of genome by graph where each nucleotide was node, and an edge was create if they follow in the genome. This type of structure could be useful to solve the limitation of reference genome approach, for example \toolsname{WhatsHap} \cite{whatshap} needs to build an alternative local part of genome according to know variant to perform remapping of read and confirmed the read dataset contains effectively the reads dataset. Similar structure was used for genome comparison \toolsname{Cactus} \cite{cactus_graph}.

This type of structure seems promising for future bioinformatics analysis, variant detection and phasing, genome comparison,  genomics evolution, and variation analysis \cite{goodbye_ref_hello_graphs}. But some trouble style need to be addressed, how to build this type of graph, map reads against them construct an efficient coordinate system, some blog post was you can read some blog post about part of this trouble \footnote{\url{http://ekg.github.io/2019/07/09/Untangling-graphical-pangenomics}} \footnote{\url{https://lh3.github.io/2019/07/08/on-a-reference-pan-genome-model}} \footnote{\url{https://lh3.github.io/2019/07/12/on-a-reference-pan-genome-model-part-ii}}. Another challenge that interests me a lot would be to be able to build a graph genome during assembly.

By using the contigs generate by assembly tools has scaffold of genome graph and \knot \OLC information I think we can generate directly the genome graph from the reads. If reads come from a single homozygote individual, this genome graph didn't contains any information, in theory, but for an heterozygote individual or a set of divergente cell like a cancer or an bacterial population, this genome graph representation can help to have a better understanding of sequenced genome.

\onlyinsubfile{
\bibliographystyle{plainnat}
\bibliography{main}
\addcontentsline{toc}{chapter}{Bibliography}
}

\end{document}\pim{Synthetis√© les algo}

